import pyspark
2
from pyspark.sql import SparkSession
3
spark = SparkSession.builder.getOrCreate()
4
from sklearn.model_selection import train_test_split
5
data=spark.read.csv('FileStore/tables/ml/Bike_Rental_UCI_dataset.csv',inferSchema=True,header=True)
6
data.dtypes
7
data.show(truncate=False)
8
#data.head()
9
#train, test = data.randomSplit([0.7, 0.3])
10
#test.show()
11
#train.cache()
12
#data.describe()
(3) Spark Jobs
data:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 11 more fields]
+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+
|season|yr |mnth|hr |holiday|workingday|weathersit|temp|hum |windspeed|dayOfWeek|days|demand|
+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+
|1     |0  |1   |0  |0      |0         |1         |0.24|0.81|0.0      |Sat      |0   |16    |
|1     |0  |1   |1  |0      |0         |1         |0.22|0.8 |0.0      |Sat      |0   |40    |
|1     |0  |1   |2  |0      |0         |1         |0.22|0.8 |0.0      |Sat      |0   |32    |
|1     |0  |1   |3  |0      |0         |1         |0.24|0.75|0.0      |Sat      |0   |13    |
|1     |0  |1   |4  |0      |0         |1         |0.24|0.75|0.0      |Sat      |0   |1     |
|1     |0  |1   |5  |0      |0         |2         |0.24|0.75|0.0896   |Sat      |0   |1     |
|1     |0  |1   |6  |0      |0         |1         |0.22|0.8 |0.0      |Sat      |0   |2     |
|1     |0  |1   |7  |0      |0         |1         |0.2 |0.86|0.0      |Sat      |0   |3     |
|1     |0  |1   |8  |0      |0         |1         |0.24|0.75|0.0      |Sat      |0   |8     |
|1     |0  |1   |9  |0      |0         |1         |0.32|0.76|0.0      |Sat      |0   |14    |
|1     |0  |1   |10 |0      |0         |1         |0.38|0.76|0.2537   |Sat      |0   |36    |
|1     |0  |1   |11 |0      |0         |1         |0.36|0.81|0.2836   |Sat      |0   |56    |
|1     |0  |1   |12 |0      |0         |1         |0.42|0.77|0.2836   |Sat      |0   |84    |
|1     |0  |1   |13 |0      |0         |2         |0.46|0.72|0.2985   |Sat      |0   |94    |
|1     |0  |1   |14 |0      |0         |2         |0.46|0.72|0.2836   |Sat      |0   |106   |
|1     |0  |1   |15 |0      |0         |2         |0.44|0.77|0.2985   |Sat      |0   |110   |
|1     |0  |1   |16 |0      |0         |2         |0.42|0.82|0.2985   |Sat      |0   |93    |
|1     |0  |1   |17 |0      |0         |2         |0.44|0.82|0.2836   |Sat      |0   |67    |
|1     |0  |1   |18 |0      |0         |3         |0.42|0.88|0.2537   |Sat      |0   |35    |
|1     |0  |1   |19 |0      |0         |3         |0.42|0.88|0.2537   |Sat      |0   |37    |
+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+
only showing top 20 rows

Command took 0.89 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:34:34 on My Cluster
1
from pyspark.ml.feature import StringIndexer
2
data = StringIndexer(
3
    inputCol='dayOfWeek', 
4
    outputCol='dayOfWeekN', 
5
    handleInvalid='keep').fit(data).transform(data)
6
data.show()
(2) Spark Jobs
data:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 12 more fields]
+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+----------+
|season| yr|mnth| hr|holiday|workingday|weathersit|temp| hum|windspeed|dayOfWeek|days|demand|dayOfWeekN|
+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+----------+
|     1|  0|   1|  0|      0|         0|         1|0.24|0.81|      0.0|      Sat|   0|    16|       0.0|
|     1|  0|   1|  1|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|    40|       0.0|
|     1|  0|   1|  2|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|    32|       0.0|
|     1|  0|   1|  3|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|    13|       0.0|
|     1|  0|   1|  4|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|     1|       0.0|
|     1|  0|   1|  5|      0|         0|         2|0.24|0.75|   0.0896|      Sat|   0|     1|       0.0|
|     1|  0|   1|  6|      0|         0|         1|0.22| 0.8|      0.0|      Sat|   0|     2|       0.0|
|     1|  0|   1|  7|      0|         0|         1| 0.2|0.86|      0.0|      Sat|   0|     3|       0.0|
|     1|  0|   1|  8|      0|         0|         1|0.24|0.75|      0.0|      Sat|   0|     8|       0.0|
|     1|  0|   1|  9|      0|         0|         1|0.32|0.76|      0.0|      Sat|   0|    14|       0.0|
|     1|  0|   1| 10|      0|         0|         1|0.38|0.76|   0.2537|      Sat|   0|    36|       0.0|
|     1|  0|   1| 11|      0|         0|         1|0.36|0.81|   0.2836|      Sat|   0|    56|       0.0|
|     1|  0|   1| 12|      0|         0|         1|0.42|0.77|   0.2836|      Sat|   0|    84|       0.0|
|     1|  0|   1| 13|      0|         0|         2|0.46|0.72|   0.2985|      Sat|   0|    94|       0.0|
|     1|  0|   1| 14|      0|         0|         2|0.46|0.72|   0.2836|      Sat|   0|   106|       0.0|
|     1|  0|   1| 15|      0|         0|         2|0.44|0.77|   0.2985|      Sat|   0|   110|       0.0|
|     1|  0|   1| 16|      0|         0|         2|0.42|0.82|   0.2985|      Sat|   0|    93|       0.0|
|     1|  0|   1| 17|      0|         0|         2|0.44|0.82|   0.2836|      Sat|   0|    67|       0.0|
|     1|  0|   1| 18|      0|         0|         3|0.42|0.88|   0.2537|      Sat|   0|    35|       0.0|
|     1|  0|   1| 19|      0|         0|         3|0.42|0.88|   0.2537|      Sat|   0|    37|       0.0|
+------+---+----+---+-------+----------+----------+----+----+---------+---------+----+------+----------+
only showing top 20 rows

Command took 0.56 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:34:39 on My Cluster
1
data = data.drop('dayOfWeek')
2
data.show()
(1) Spark Jobs
data:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 11 more fields]
+------+---+----+---+-------+----------+----------+----+----+---------+----+------+----------------+
|season| yr|mnth| hr|holiday|workingday|weathersit|temp| hum|windspeed|days|demand|dayOfWeekNumeric|
+------+---+----+---+-------+----------+----------+----+----+---------+----+------+----------------+
|     1|  0|   1|  0|      0|         0|         1|0.24|0.81|      0.0|   0|    16|             0.0|
|     1|  0|   1|  1|      0|         0|         1|0.22| 0.8|      0.0|   0|    40|             0.0|
|     1|  0|   1|  2|      0|         0|         1|0.22| 0.8|      0.0|   0|    32|             0.0|
|     1|  0|   1|  3|      0|         0|         1|0.24|0.75|      0.0|   0|    13|             0.0|
|     1|  0|   1|  4|      0|         0|         1|0.24|0.75|      0.0|   0|     1|             0.0|
|     1|  0|   1|  5|      0|         0|         2|0.24|0.75|   0.0896|   0|     1|             0.0|
|     1|  0|   1|  6|      0|         0|         1|0.22| 0.8|      0.0|   0|     2|             0.0|
|     1|  0|   1|  7|      0|         0|         1| 0.2|0.86|      0.0|   0|     3|             0.0|
|     1|  0|   1|  8|      0|         0|         1|0.24|0.75|      0.0|   0|     8|             0.0|
|     1|  0|   1|  9|      0|         0|         1|0.32|0.76|      0.0|   0|    14|             0.0|
|     1|  0|   1| 10|      0|         0|         1|0.38|0.76|   0.2537|   0|    36|             0.0|
|     1|  0|   1| 11|      0|         0|         1|0.36|0.81|   0.2836|   0|    56|             0.0|
|     1|  0|   1| 12|      0|         0|         1|0.42|0.77|   0.2836|   0|    84|             0.0|
|     1|  0|   1| 13|      0|         0|         2|0.46|0.72|   0.2985|   0|    94|             0.0|
|     1|  0|   1| 14|      0|         0|         2|0.46|0.72|   0.2836|   0|   106|             0.0|
|     1|  0|   1| 15|      0|         0|         2|0.44|0.77|   0.2985|   0|   110|             0.0|
|     1|  0|   1| 16|      0|         0|         2|0.42|0.82|   0.2985|   0|    93|             0.0|
|     1|  0|   1| 17|      0|         0|         2|0.44|0.82|   0.2836|   0|    67|             0.0|
|     1|  0|   1| 18|      0|         0|         3|0.42|0.88|   0.2537|   0|    35|             0.0|
|     1|  0|   1| 19|      0|         0|         3|0.42|0.88|   0.2537|   0|    37|             0.0|
+------+---+----+---+-------+----------+----------+----+----+---------+----+------+----------------+
only showing top 20 rows

Command took 0.55 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:50 on My Cluster
1
required_features = ['season',
2
                     'yr',
3
                     'mnth',
4
                     'hr',
5
                     'holiday',
6
                     'workingday',
7
                     'weathersit',
8
                     'temp',
9
                     'hum',
10
                     'windspeed',
11
                     'days',
12
                     'dayOfWeekNumeric'
13
    
14
                   ]
15
from pyspark.ml.feature import VectorAssembler
16
assembler = VectorAssembler(inputCols=required_features, outputCol='features')
17
transformed_data = assembler.transform(data)
18
transformed_data.show()
(1) Spark Jobs
transformed_data:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 12 more fields]
+------+---+----+---+-------+----------+----------+----+----+---------+----+------+----------------+--------------------+
|season| yr|mnth| hr|holiday|workingday|weathersit|temp| hum|windspeed|days|demand|dayOfWeekNumeric|            features|
+------+---+----+---+-------+----------+----------+----+----+---------+----+------+----------------+--------------------+
|     1|  0|   1|  0|      0|         0|         1|0.24|0.81|      0.0|   0|    16|             0.0|(12,[0,2,6,7,8],[...|
|     1|  0|   1|  1|      0|         0|         1|0.22| 0.8|      0.0|   0|    40|             0.0|(12,[0,2,3,6,7,8]...|
|     1|  0|   1|  2|      0|         0|         1|0.22| 0.8|      0.0|   0|    32|             0.0|(12,[0,2,3,6,7,8]...|
|     1|  0|   1|  3|      0|         0|         1|0.24|0.75|      0.0|   0|    13|             0.0|(12,[0,2,3,6,7,8]...|
|     1|  0|   1|  4|      0|         0|         1|0.24|0.75|      0.0|   0|     1|             0.0|(12,[0,2,3,6,7,8]...|
|     1|  0|   1|  5|      0|         0|         2|0.24|0.75|   0.0896|   0|     1|             0.0|[1.0,0.0,1.0,5.0,...|
|     1|  0|   1|  6|      0|         0|         1|0.22| 0.8|      0.0|   0|     2|             0.0|(12,[0,2,3,6,7,8]...|
|     1|  0|   1|  7|      0|         0|         1| 0.2|0.86|      0.0|   0|     3|             0.0|(12,[0,2,3,6,7,8]...|
|     1|  0|   1|  8|      0|         0|         1|0.24|0.75|      0.0|   0|     8|             0.0|(12,[0,2,3,6,7,8]...|
|     1|  0|   1|  9|      0|         0|         1|0.32|0.76|      0.0|   0|    14|             0.0|(12,[0,2,3,6,7,8]...|
|     1|  0|   1| 10|      0|         0|         1|0.38|0.76|   0.2537|   0|    36|             0.0|[1.0,0.0,1.0,10.0...|
|     1|  0|   1| 11|      0|         0|         1|0.36|0.81|   0.2836|   0|    56|             0.0|[1.0,0.0,1.0,11.0...|
|     1|  0|   1| 12|      0|         0|         1|0.42|0.77|   0.2836|   0|    84|             0.0|[1.0,0.0,1.0,12.0...|
|     1|  0|   1| 13|      0|         0|         2|0.46|0.72|   0.2985|   0|    94|             0.0|[1.0,0.0,1.0,13.0...|
|     1|  0|   1| 14|      0|         0|         2|0.46|0.72|   0.2836|   0|   106|             0.0|[1.0,0.0,1.0,14.0...|
|     1|  0|   1| 15|      0|         0|         2|0.44|0.77|   0.2985|   0|   110|             0.0|[1.0,0.0,1.0,15.0...|
|     1|  0|   1| 16|      0|         0|         2|0.42|0.82|   0.2985|   0|    93|             0.0|[1.0,0.0,1.0,16.0...|
|     1|  0|   1| 17|      0|         0|         2|0.44|0.82|   0.2836|   0|    67|             0.0|[1.0,0.0,1.0,17.0...|
|     1|  0|   1| 18|      0|         0|         3|0.42|0.88|   0.2537|   0|    35|             0.0|[1.0,0.0,1.0,18.0...|
|     1|  0|   1| 19|      0|         0|         3|0.42|0.88|   0.2537|   0|    37|             0.0|[1.0,0.0,1.0,19.0...|
+------+---+----+---+-------+----------+----------+----+----+---------+----+------+----------------+--------------------+
only showing top 20 rows

Command took 1.02 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:50 on My Cluster
1
(training_data, test_data) = transformed_data.randomSplit([0.8,0.2])
2
training_data.show()
(1) Spark Jobs
training_data:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 12 more fields]
test_data:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 12 more fields]
+------+---+----+---+-------+----------+----------+----+----+-------------------+----+------+----------------+--------------------+
|season| yr|mnth| hr|holiday|workingday|weathersit|temp| hum|          windspeed|days|demand|dayOfWeekNumeric|            features|
+------+---+----+---+-------+----------+----------+----+----+-------------------+----+------+----------------+--------------------+
|     1|  0|   1|  0|      0|         0|         1|0.04|0.45|             0.2537|  19|    13|             0.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         0|         1| 0.1|0.42|             0.3881|   7|    25|             1.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         0|         1|0.16| 0.8|             0.1045|  26|    33|             1.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         0|         1|0.18|0.55|                0.0|  13|    28|             0.0|(12,[0,2,6,7,8,10...|
|     1|  0|   1|  0|      0|         0|         1|0.22|0.64|             0.3582|  25|    28|             0.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         0|         1|0.26|0.56|                0.0|  14|    39|             1.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         0|         2|0.18|0.51|             0.1642|   6|    25|             0.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         0|         2|0.46|0.88|             0.2985|   1|    17|             1.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         1|0.12| 0.5|0.19399999999999998|  12|    14|             2.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         1|0.12| 0.5|             0.2836|   8|     5|             3.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         1|0.14|0.59|             0.1045|   9|    12|             6.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         1|0.14|0.59|             0.2836|  11|     7|             5.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         1|0.16|0.55|             0.1045|   2|     5|             6.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         1|0.18|0.55|                0.0|   4|    11|             5.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         1| 0.2|0.64|                0.0|   3|     6|             4.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         1|0.22|0.44|             0.3582|   1|     5|             3.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         2|0.16|0.86|             0.0896|  10|     7|             4.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         2| 0.2|0.64|0.19399999999999998|   5|    17|             2.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         2| 0.2|0.75|             0.1343|  24|     9|             2.0|[1.0,0.0,1.0,0.0,...|
|     1|  0|   1|  0|      0|         1|         2|0.22|0.93|                0.0|  17|     3|             4.0|[1.0,0.0,1.0,0.0,...|
+------+---+----+---+-------+----------+----------+----+----+-------------------+----+------+----------------+--------------------+
only showing top 20 rows

Command took 1.96 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:50 on My Cluster
1
from pyspark.ml.classification import RandomForestClassifier
2
rf = RandomForestClassifier(labelCol='demand', 
3
                            featuresCol='features',
4
                            maxDepth=12)
Command took 0.09 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:50 on My Cluster
1
model = rf.fit(training_data)
(1) Spark Jobs
java.lang.IllegalArgumentException: requirement failed: Classifier inferred 977 from label values in column RandomForestClassifier_583d56b67b6f__labelCol, but this exceeded the max numClasses (100) allowed to be inferred from values.  To avoid this error for labels with &gt; 100 classes, specify numClasses explicitly in the metadata; this can be done by applying StringIndexer to the label column.
Command took 9.15 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:50 on My Cluster
1
 
1
dtm = dt.fit(training_data)
2
dtm_results = dtm.transform(test_data)
NameError: name 'dt' is not defined
Command took 9.36 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:51 on My Cluster
1
#from pyspark.ml.classification import (LogisticRegression, 
2
                                       #DecisionTreeClassifier, 
3
                                       #RandomForestClassifier
4
                                      #)
5
#y = data.demand
6
#x = data.drop('demand')
7
#x.show()
8
#print(y)
9
#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)
10
#x_train.head()
11
#x_test.head()
12
#x_test.shape
13
cols=data.columns
14
cols.remove("demand")
15
# Let us import the vector assembler
16
from pyspark.ml.feature import VectorAssembler
17
assembler = VectorAssembler(inputCols=cols,outputCol="features")
18
# Now let us use the transform method to transform our dataset
19
data=assembler.transform(data)
20
#print(data)
21
data.select("features").show(truncate=False)
22
 
(1) Spark Jobs
data:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 12 more fields]
+-------------------------------------------------------+
|features                                               |
+-------------------------------------------------------+
|(12,[0,2,6,7,8],[1.0,1.0,1.0,0.24,0.81])               |
|(12,[0,2,3,6,7,8],[1.0,1.0,1.0,1.0,0.22,0.8])          |
|(12,[0,2,3,6,7,8],[1.0,1.0,2.0,1.0,0.22,0.8])          |
|(12,[0,2,3,6,7,8],[1.0,1.0,3.0,1.0,0.24,0.75])         |
|(12,[0,2,3,6,7,8],[1.0,1.0,4.0,1.0,0.24,0.75])         |
|[1.0,0.0,1.0,5.0,0.0,0.0,2.0,0.24,0.75,0.0896,0.0,0.0] |
|(12,[0,2,3,6,7,8],[1.0,1.0,6.0,1.0,0.22,0.8])          |
|(12,[0,2,3,6,7,8],[1.0,1.0,7.0,1.0,0.2,0.86])          |
|(12,[0,2,3,6,7,8],[1.0,1.0,8.0,1.0,0.24,0.75])         |
|(12,[0,2,3,6,7,8],[1.0,1.0,9.0,1.0,0.32,0.76])         |
|[1.0,0.0,1.0,10.0,0.0,0.0,1.0,0.38,0.76,0.2537,0.0,0.0]|
|[1.0,0.0,1.0,11.0,0.0,0.0,1.0,0.36,0.81,0.2836,0.0,0.0]|
|[1.0,0.0,1.0,12.0,0.0,0.0,1.0,0.42,0.77,0.2836,0.0,0.0]|
|[1.0,0.0,1.0,13.0,0.0,0.0,2.0,0.46,0.72,0.2985,0.0,0.0]|
|[1.0,0.0,1.0,14.0,0.0,0.0,2.0,0.46,0.72,0.2836,0.0,0.0]|
|[1.0,0.0,1.0,15.0,0.0,0.0,2.0,0.44,0.77,0.2985,0.0,0.0]|
|[1.0,0.0,1.0,16.0,0.0,0.0,2.0,0.42,0.82,0.2985,0.0,0.0]|
|[1.0,0.0,1.0,17.0,0.0,0.0,2.0,0.44,0.82,0.2836,0.0,0.0]|
|[1.0,0.0,1.0,18.0,0.0,0.0,3.0,0.42,0.88,0.2537,0.0,0.0]|
|[1.0,0.0,1.0,19.0,0.0,0.0,3.0,0.42,0.88,0.2537,0.0,0.0]|
+-------------------------------------------------------+
only showing top 20 rows

Command took 0.70 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:51 on My Cluster
1
 
1
from pyspark.ml.feature import StandardScaler
2
standardscaler=StandardScaler().setInputCol("features").setOutputCol("Scaled_features")
3
data=standardscaler.fit(data).transform(data)
4
data.select("features","Scaled_features").show(5)
(2) Spark Jobs
data:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 13 more fields]
+--------------------+--------------------+
|            features|     Scaled_features|
+--------------------+--------------------+
|(12,[0,2,6,7,8],[...|(12,[0,2,6,7,8],[...|
|(12,[0,2,3,6,7,8]...|(12,[0,2,3,6,7,8]...|
|(12,[0,2,3,6,7,8]...|(12,[0,2,3,6,7,8]...|
|(12,[0,2,3,6,7,8]...|(12,[0,2,3,6,7,8]...|
|(12,[0,2,3,6,7,8]...|(12,[0,2,3,6,7,8]...|
+--------------------+--------------------+
only showing top 5 rows

Command took 1.85 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:51 on My Cluster
1
train, test = data.randomSplit([0.8, 0.2], seed=12345)
2
from pyspark.ml.feature import ChiSqSelector
3
css = ChiSqSelector(featuresCol='Scaled_features',outputCol='Aspect',labelCol='demand',fpr=0.05)
4
train=css.fit(train).transform(train)
5
test=css.fit(test).transform(test)
6
test.select("Aspect").show(5,truncate=False)
7
#from pyspark.ml.classification import LogisticRegression
8
#lr = LogisticRegression(labelCol="demand", featuresCol="Aspect",weightCol="classWeights",maxIter=10)
9
#model=lr.fit(train)
10
#predict_train=model.transform(train)
11
#predict_test=model.transform(test)
12
#predict_test.select("demand","prediction").show(10)
13
 
(5) Spark Jobs
train:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 14 more fields]
test:pyspark.sql.dataframe.DataFrame = [season: integer, yr: integer ... 14 more fields]
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|Aspect                                                                                                                                                                                    |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|(12,[0,2,6,7,8,10],[0.9034091721530642,0.29080116973067877,1.5640717020398847,0.9347924066592604,2.850777344371965,0.062188213491837924])                                                 |
|[0.9034091721530642,0.0,0.29080116973067877,0.0,0.0,0.0,1.5640717020398847,1.3502556985078207,2.902609659724183,0.0,0.06697192222197931,0.49991595341755574]                              |
|[0.9034091721530642,0.0,0.29080116973067877,0.0,0.0,0.0,3.1281434040797693,0.9347924066592604,2.643448082963095,1.3421586825163085,0.028702252380848274,0.0]                              |
|[0.9034091721530642,0.0,0.29080116973067877,0.0,0.0,2.1485478779700613,1.5640717020398847,0.8309265836971205,2.850777344371965,0.8541752882031317,0.009567417460282758,2.9994957205053345]|
|[0.9034091721530642,0.0,0.29080116973067877,0.0,0.0,2.1485478779700613,1.5640717020398847,0.9347924066592604,2.850777344371965,0.0,0.019134834920565516,2.499579767087779]                |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
only showing top 5 rows

Command took 12.76 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:51 on My Cluster
1
dt.explainParams()
2
lrm = lr.fit(train)
3
dtm = dt.fit(train)
4
rfm = rf.fit(train)
5
lrm_results = lrm.transform(test)
6
dtm_results = dtm.transform(test)
7
rfm_results = rfm.transform(test)
8
evaluation_lrm=evaluator.evaluate(lrm_results)
9
evaluation_dtm=evaluator.evaluate(dtm_results)
10
evaluation_rfm=evaluator.evaluate(rfm_results)
11
print('evaluation of logistic regression model = %g'%evaluation_lrm)
12
print('evaluation of decision tree model = %g'%evaluation_dtm)
13
print('evaluation of random forest model = %g'%evaluation_rfm)
NameError: name 'dt' is not defined
Command took 24.58 seconds -- by meyissi1229@gmail.com at 2020/7/16 下午10:20:51 on My Cluster
